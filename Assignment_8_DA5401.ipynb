{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Part A"
      ],
      "metadata": {
        "id": "25XML54l3l8t"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b07a4ef"
      },
      "source": [
        "### Data Loading and Feature Engineering\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5c8ca21a",
        "outputId": "0c9639fc-5d22-4f63-ca7c-1e0fa65fb18f"
      },
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "\n",
        "zip_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00275/Bike-Sharing-Dataset.zip'\n",
        "filename = 'hour.csv'\n",
        "temp_dir = 'temp_data'\n",
        "zip_file_path = os.path.join(temp_dir, 'Bike-Sharing-Dataset.zip')\n",
        "\n",
        "\n",
        "os.makedirs(temp_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "if not os.path.exists(zip_file_path):\n",
        "    print(f\"Downloading {zip_url}...\")\n",
        "    response = requests.get(zip_url)\n",
        "    response.raise_for_status()\n",
        "    with open(zip_file_path, 'wb') as f:\n",
        "        f.write(response.content)\n",
        "    print(\"Zip file downloaded successfully.\")\n",
        "else:\n",
        "    print(\"Zip file already exists, skipping download.\")\n",
        "\n",
        "\n",
        "print(f\"Extracting {zip_file_path}...\")\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(temp_dir)\n",
        "print(\"Zip file extracted successfully.\")\n",
        "\n",
        "\n",
        "df = pd.read_csv(os.path.join(temp_dir, filename))\n",
        "\n",
        "\n",
        "df.drop(['instant', 'dteday', 'casual', 'registered'], axis=1, inplace=True)\n",
        "\n",
        "\n",
        "categorical_cols = ['season', 'yr', 'mnth', 'hr', 'weekday', 'weathersit', 'holiday']\n",
        "\n",
        "\n",
        "df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)\n",
        "\n",
        "print(\"DataFrame after loading, dropping columns, and one-hot encoding:\")\n",
        "print(df.head())\n",
        "print(\"Shape of the DataFrame:\", df.shape)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://archive.ics.uci.edu/ml/machine-learning-databases/00275/Bike-Sharing-Dataset.zip...\n",
            "Zip file downloaded successfully.\n",
            "Extracting temp_data/Bike-Sharing-Dataset.zip...\n",
            "Zip file extracted successfully.\n",
            "DataFrame after loading, dropping columns, and one-hot encoding:\n",
            "   workingday  temp   atemp   hum  windspeed  cnt  season_2  season_3  \\\n",
            "0           0  0.24  0.2879  0.81        0.0   16     False     False   \n",
            "1           0  0.22  0.2727  0.80        0.0   40     False     False   \n",
            "2           0  0.22  0.2727  0.80        0.0   32     False     False   \n",
            "3           0  0.24  0.2879  0.75        0.0   13     False     False   \n",
            "4           0  0.24  0.2879  0.75        0.0    1     False     False   \n",
            "\n",
            "   season_4   yr_1  ...  weekday_1  weekday_2  weekday_3  weekday_4  \\\n",
            "0     False  False  ...      False      False      False      False   \n",
            "1     False  False  ...      False      False      False      False   \n",
            "2     False  False  ...      False      False      False      False   \n",
            "3     False  False  ...      False      False      False      False   \n",
            "4     False  False  ...      False      False      False      False   \n",
            "\n",
            "   weekday_5  weekday_6  weathersit_2  weathersit_3  weathersit_4  holiday_1  \n",
            "0      False       True         False         False         False      False  \n",
            "1      False       True         False         False         False      False  \n",
            "2      False       True         False         False         False      False  \n",
            "3      False       True         False         False         False      False  \n",
            "4      False       True         False         False         False      False  \n",
            "\n",
            "[5 rows x 54 columns]\n",
            "Shape of the DataFrame: (17379, 54)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bd0eb72"
      },
      "source": [
        "### Train/Test Split\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4b16ef82",
        "outputId": "ad169b82-ccfc-4500-c702-50aa5443a31e"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "X = df.drop('cnt', axis=1)\n",
        "y = df['cnt']\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Shape of X_train:\", X_train.shape)\n",
        "print(\"Shape of X_test:\", X_test.shape)\n",
        "print(\"Shape of y_train:\", y_train.shape)\n",
        "print(\"Shape of y_test:\", y_test.shape)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X_train: (13903, 53)\n",
            "Shape of X_test: (3476, 53)\n",
            "Shape of y_train: (13903,)\n",
            "Shape of y_test: (3476,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fede53f"
      },
      "source": [
        "### Baseline Model Training and Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "743c7166",
        "outputId": "49636283-c155-4abb-eff4-9b7cbf86ee99"
      },
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "dtr_model = DecisionTreeRegressor(max_depth=6, random_state=42)\n",
        "\n",
        "\n",
        "dtr_model.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "dtr_predictions = dtr_model.predict(X_test)\n",
        "\n",
        "\n",
        "dtr_rmse = np.sqrt(mean_squared_error(y_test, dtr_predictions))\n",
        "\n",
        "lr_model = LinearRegression()\n",
        "\n",
        "\n",
        "lr_model.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "lr_predictions = lr_model.predict(X_test)\n",
        "\n",
        "\n",
        "lr_rmse = np.sqrt(mean_squared_error(y_test, lr_predictions))\n",
        "\n",
        "\n",
        "print(f\"Decision Tree Regressor RMSE: {dtr_rmse:.4f}\")\n",
        "print(f\"Linear Regression RMSE: {lr_rmse:.4f}\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Regressor RMSE: 118.4555\n",
            "Linear Regression RMSE: 100.4459\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part B"
      ],
      "metadata": {
        "id": "UeNx4Ojk4CAp"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1688982"
      },
      "source": [
        "## Bagging (Variance Reduction)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "859cf8c1",
        "outputId": "de4df75c-61de-4ab7-a0e2-b55c702e424d"
      },
      "source": [
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "decision_tree_base = DecisionTreeRegressor(max_depth=6, random_state=42)\n",
        "\n",
        "bagging_model = BaggingRegressor(estimator=decision_tree_base, n_estimators=50, random_state=42, n_jobs=-1)\n",
        "\n",
        "bagging_model.fit(X_train, y_train)\n",
        "\n",
        "bagging_predictions = bagging_model.predict(X_test)\n",
        "\n",
        "bagging_rmse = np.sqrt(mean_squared_error(y_test, bagging_predictions))\n",
        "\n",
        "print(f\"Bagging Regressor RMSE: {bagging_rmse:.4f}\")\n",
        "print(f\"Single Decision Tree Regressor RMSE (baseline for comparison): {dtr_rmse:.4f}\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Regressor RMSE: 112.3521\n",
            "Single Decision Tree Regressor RMSE (baseline for comparison): 118.4555\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15baf625"
      },
      "source": [
        "### Discussion: Boosting (Bias Reduction)\n",
        "\n",
        "The Gradient Boosting Regressor achieved an RMSE of **78.9652**. This is significantly lower than the RMSE of the single Decision Tree Regressor (**118.4555**), the Linear Regression baseline (**100.4459**), and even the Bagging Regressor (**112.3521**). The substantial improvement in RMSE with Gradient Boosting demonstrates its effectiveness in achieving a better result than both the single models and the bagging ensemble.\n",
        "\n",
        "This outcome strongly supports the hypothesis that boosting primarily targets bias reduction. Boosting works by sequentially building models, where each new model attempts to correct the errors (residuals) of the previous models. By focusing on mispredicted instances and gradually improving the model's accuracy, boosting effectively reduces the overall bias of the ensemble. The fact that Gradient Boosting outperformed the relatively simpler models and even the variance-reducing Bagging technique highlights its power in capturing complex patterns and reducing systematic errors, thereby leading to a more accurate and less biased prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88ca28b4"
      },
      "source": [
        "## Boosting (Bias Reduction)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dd21498",
        "outputId": "b6aa1332-7a07-49bd-d081-0dba866666c3"
      },
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "gbr_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "\n",
        "gbr_model.fit(X_train, y_train)\n",
        "\n",
        "gbr_predictions = gbr_model.predict(X_test)\n",
        "\n",
        "gbr_rmse = np.sqrt(mean_squared_error(y_test, gbr_predictions))\n",
        "\n",
        "print(f\"Gradient Boosting Regressor RMSE: {gbr_rmse:.4f}\")\n",
        "print(f\"Single Decision Tree Regressor RMSE: {dtr_rmse:.4f}\")\n",
        "print(f\"Linear Regression RMSE (Baseline): {lr_rmse:.4f}\")\n",
        "print(f\"Bagging Regressor RMSE: {bagging_rmse:.4f}\")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient Boosting Regressor RMSE: 78.9652\n",
            "Single Decision Tree Regressor RMSE: 118.4555\n",
            "Linear Regression RMSE (Baseline): 100.4459\n",
            "Bagging Regressor RMSE: 112.3521\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff57b4d6"
      },
      "source": [
        "### Discussion: Bagging (Variance Reduction)\n",
        "\n",
        "The Bagging Regressor achieved an RMSE of **112.3521**, which is notably lower than the RMSE of the single Decision Tree Regressor (**118.4555**). This reduction in RMSE suggests that the bagging technique was effective in reducing the variance of the model. Bagging works by training multiple base estimators on different subsets of the training data (bootstrap samples) and then averaging their predictions. This process helps to reduce the impact of individual noisy predictions from highly variant models like Decision Trees, thereby smoothing out the overall prediction and reducing variance without significantly increasing bias. The improvement observed here aligns with the hypothesis that bagging primarily targets variance reduction."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part C"
      ],
      "metadata": {
        "id": "H9r56WA94gRQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bba66a5"
      },
      "source": [
        "## Stacking Principle Explained\n",
        "\n",
        "Stacking (Stacked Generalization) is an advanced ensemble learning technique that combines multiple diverse models, called **Base Learners**, and uses another model, called a **Meta-Learner** (or blender), to learn how to optimally combine their predictions. The core idea behind Stacking is to harness the strengths of different models by allowing a meta-learner to make the final prediction based on the outputs of the base learners.\n",
        "\n",
        "### Role of Base Learners\n",
        "\n",
        "Base Learners are the individual models that are trained on the original dataset (or subsets of it). These models can be of different types (e.g., Decision Trees, Linear Regression, Support Vector Machines, Neural Networks) and are chosen for their diverse predictive capabilities. Each base learner makes its own set of predictions on the data. For instance, in a regression problem, each base learner would output a predicted numerical value.\n",
        "\n",
        "### Role of the Meta-Learner\n",
        "\n",
        "The Meta-Learner is a model that is trained on the *predictions* generated by the base learners, rather than directly on the original features. The predictions from the base learners serve as the input features for the meta-learner. The meta-learner then learns to weigh or combine these predictions to produce a final, more robust prediction. A common practice is to use a simple model like Linear Regression, Logistic Regression, or a Ridge Classifier as the meta-learner to prevent overfitting, but more complex models can also be used.\n",
        "\n",
        "### How it Works (in a nutshell):\n",
        "\n",
        "1.  **Training Base Learners**: Multiple diverse base models are trained on the training data.\n",
        "2.  **Generating Meta-Features**: Each base learner makes predictions on a *different fold* of the training data (e.g., using k-fold cross-validation) to avoid data leakage. These predictions become the input features for the meta-learner. The base learners also make predictions on the test set.\n",
        "3.  **Training Meta-Learner**: The meta-learner is trained on the meta-features (predictions from base learners) and the true labels of the training data.\n",
        "4.  **Final Prediction**: For new, unseen data, the base learners first make their predictions. These predictions are then fed into the trained meta-learner, which produces the final output.\n",
        "\n",
        "### Advantages of Stacking\n",
        "\n",
        "-   **Leverages Diverse Strengths**: Stacking effectively combines the unique predictive powers of different base models. If one model is good at capturing certain patterns and another excels at different ones, the meta-learner can learn to optimally utilize both.\n",
        "-   **Improved Performance**: By learning how to best combine predictions, Stacking often leads to better predictive performance (e.g., lower RMSE for regression or higher accuracy for classification) compared to individual base models or simpler ensemble methods like Bagging or Boosting.\n",
        "-   **Reduced Bias and Variance**: It can help in reducing both bias (by combining strong learners) and variance (by averaging out errors, similar to bagging, but with an intelligent weighted combination).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e28ec845"
      },
      "source": [
        "## Implement Stacking Regressor\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83214e9d",
        "outputId": "8d81a4f5-507c-4f58-c20c-d594fbda8d16"
      },
      "source": [
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.ensemble import StackingRegressor, BaggingRegressor, GradientBoostingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "knn_model = KNeighborsRegressor(n_neighbors=5)\n",
        "\n",
        "bagging_base_estimator = DecisionTreeRegressor(max_depth=6, random_state=42)\n",
        "bagging_model = BaggingRegressor(estimator=bagging_base_estimator, n_estimators=50, random_state=42, n_jobs=-1)\n",
        "\n",
        "gbr_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "\n",
        "meta_learner = Ridge(random_state=42)\n",
        "\n",
        "estimation = [\n",
        "    ('knn', knn_model),\n",
        "    ('bagging', bagging_model),\n",
        "    ('gbr', gbr_model)\n",
        "]\n",
        "\n",
        "stacking_regressor = StackingRegressor(estimators=estimation, final_estimator=meta_learner, n_jobs=-1)\n",
        "\n",
        "stacking_regressor.fit(X_train, y_train)\n",
        "\n",
        "print(\"Stacking Regressor initialized and fitted.\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stacking Regressor initialized and fitted.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ad97043a"
      },
      "source": [
        "## Evaluate Stacking Regressor\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ce13f24",
        "outputId": "7abe3129-4194-41af-d41b-055eb22bda92"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "stacking_predictions = stacking_regressor.predict(X_test)\n",
        "\n",
        "stacking_rmse = np.sqrt(mean_squared_error(y_test, stacking_predictions))\n",
        "\n",
        "print(f\"Stacking Regressor RMSE: {stacking_rmse:.4f}\")\n",
        "print(f\"Single Decision Tree Regressor RMSE: {dtr_rmse:.4f}\")\n",
        "print(f\"Linear Regression RMSE (Baseline): {lr_rmse:.4f}\")\n",
        "print(f\"Bagging Regressor RMSE: {bagging_rmse:.4f}\")\n",
        "print(f\"Gradient Boosting Regressor RMSE: {gbr_rmse:.4f}\")"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stacking Regressor RMSE: 67.0291\n",
            "Single Decision Tree Regressor RMSE: 118.4555\n",
            "Linear Regression RMSE (Baseline): 100.4459\n",
            "Bagging Regressor RMSE: 112.3521\n",
            "Gradient Boosting Regressor RMSE: 78.9652\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3689b64f"
      },
      "source": [
        "### Discussion: Stacking Regressor Performance\n",
        "\n",
        "The Stacking Regressor achieved an RMSE of **67.0291**. This is the lowest RMSE among all the models implemented:\n",
        "-   Single Decision Tree Regressor: **118.4555**\n",
        "-   Linear Regression (Baseline): **100.4459**\n",
        "-   Bagging Regressor: **112.3521**\n",
        "-   Gradient Boosting Regressor: **78.9652**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part D"
      ],
      "metadata": {
        "id": "8CGDQOGf4u-X"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ab5b674"
      },
      "source": [
        "### Comparative Table of Model Performance (RMSE)\n",
        "\n",
        "| Model                               | RMSE       |\n",
        "| :---------------------------------- | :--------- |\n",
        "| Single Decision Tree Regressor      | 118.4555   |\n",
        "| Linear Regression (Baseline)        | 100.4459   |\n",
        "| Bagging Regressor                   | 112.3521   |\n",
        "| Gradient Boosting Regressor         | 78.9652    |\n",
        "| **Stacking Regressor**              | **67.0291**|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11e574e8"
      },
      "source": [
        "### Conclusion\n",
        "\n",
        "Based on the Root Mean Squared Error (RMSE) values, the **Stacking Regressor** is the best-performing model, achieving an RMSE of **67.0291**. This significantly outperforms all other models, including the single models (Decision Tree and Linear Regression) and the other ensemble techniques (Bagging and Gradient Boosting).\n",
        "\n",
        "#### Why Stacking Outperformed the Single Model Baseline:\n",
        "\n",
        "The Stacking Regressor's superior performance can be attributed to its sophisticated approach to ensemble learning, which effectively leverages the principles of the bias-variance trade-off and model diversity:\n",
        "\n",
        "1.  **Bias-Variance Trade-off**: The individual base learners (K-Nearest Neighbors, Bagging Regressor, and Gradient Boosting Regressor) inherently possess different strengths and weaknesses concerning bias and variance. For instance:\n",
        "    *   **Decision Trees** (and thus the Bagging Regressor's base estimators) are high-variance, low-bias models. Bagging effectively reduces this variance by averaging the predictions of multiple diverse trees.\n",
        "    *   **Gradient Boosting** primarily targets bias reduction by sequentially building models that correct the errors of previous models, thereby reducing systematic errors.\n",
        "    *   **K-Nearest Neighbors** can be sensitive to local data structure and feature scaling, offering another distinct predictive perspective.\n",
        "    \n",
        "    Stacking allows the Meta-Learner (Ridge Regression in this case) to learn the optimal way to combine these diverse predictions. The meta-learner acts as a sophisticated weighted average or a more complex function that can 'learn' when to trust certain base models over others, effectively minimizing the combined bias and variance that the individual models or simpler ensembles might leave behind. It seeks a sweet spot in the bias-variance trade-off by intelligently integrating models that address different aspects of prediction error.\n",
        "\n",
        "2.  **Model Diversity**: The key to Stacking's success is the diversity of its base learners. By including models with fundamentally different learning paradigms (e.g., instance-based learning with KNN, tree-based ensembles with Bagging and Gradient Boosting), the Stacking Regressor captures a wider range of patterns and relationships within the data. Each base learner might excel at predicting different subsets of the data or handling different types of relationships. The meta-learner then synthesizes these varied 'opinions' to produce a more robust and accurate final prediction than any single model could achieve. This diversity ensures that the errors made by one base model are not systematically repeated by others, allowing the meta-learner to correct for them and reduce the overall prediction error."
      ]
    }
  ]
}